{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a63ffa0-4324-4e71-a319-92b5c80bb8e5",
   "metadata": {},
   "source": [
    "# Binary Classification on the MNIST dataset\n",
    "\n",
    "As an example to showcase the use of the `qiboml` Pytorch interface, we demonstrate in the following how to run a simple binary classification problem on the MNIST data. We are going to combine a classical image encoder with a quantum classification head to learn to ditinguish zeros and ones.\n",
    "\n",
    "Before that, though, let's install the required packages.\n",
    "\n",
    "## Installing Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d5a1f-8205-4da9-966b-1f69ce543983",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# qiboml, provides the means to build the quantum model\n",
    "!pip install git+https://github.com/qiboteam/qiboml;\n",
    "\n",
    "# torch, handles the creation of all the classical layers\n",
    "# and provides the optimizer\n",
    "!pip install torch;\n",
    "\n",
    "# some additional requirements to run this notebook\n",
    "# torchvision to gather the MNIST dataset\n",
    "# torchmetrics to evaluate the performance\n",
    "# matplotlib to do some plotting\n",
    "!pip install torchvision torchmetrics matplotlib;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037aae23-283a-47a0-8b8b-f9684374c8cd",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "Now, let's start by downloading the MNIST dataset. Each element of the dataset is a `28x28` black and white image representing a hand written digit from `0` to `9`. However, since we are interested in just the zeros and ones for our simplified binary classification problem, we extract from the complete dataset just the element that are labeled as either `0` or `1`. \n",
    "\n",
    "Since the problem is relatively simple, we will not need the whole dataset, but, rather, about 100 train samples should be enough to obtain a reasonable accuracy. Analogously, we also take other 100 samples for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3d6df3-99a0-46f7-a054-77cd907dd6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# get the MNIST dataset\n",
    "dataset = MNIST(\"./\", download=True)\n",
    "# each element is an image of shape 28x28\n",
    "print(f\"First element of the dataset:\\nlabel: {dataset[0][1]}\\nshape: {dataset.data[0].shape}\\nimage:\\n\")\n",
    "display(dataset[0][0])\n",
    "\n",
    "# we keep only the zeros and ones for binary classification\n",
    "zeros_and_ones = [i for i, d in enumerate(dataset) if d[1] in (0,1)]\n",
    "# for this small example 100 data for training should be enough\n",
    "# we also extract 100 other data for testing\n",
    "# in practice we take the first 200 zeros and ones found in the dataset\n",
    "train_data, test_data = dataset.data[zeros_and_ones][:200].double().view(2, 100, 1, 28, 28)\n",
    "train_targets, test_targets = dataset.targets[zeros_and_ones][:200].double().view(2, 100, 1)\n",
    "# flatten the targets\n",
    "train_targets, test_targets = train_targets.view(-1,), test_targets.view(-1,)\n",
    "train_images = [dataset[i][0] for i in zeros_and_ones][:100]\n",
    "test_images = [dataset[i][0] for i in zeros_and_ones][100:200]\n",
    "\n",
    "print(f\"\\nFirst element of our binary classification task:\\nlabel: {int(train_targets[0])}\\nimage:\\n\")\n",
    "display(train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d4c81b-65c2-4170-b0c6-b15ab4b762bc",
   "metadata": {},
   "source": [
    "## Construct the Image Encoder\n",
    "\n",
    "Now that we have prepared all the data we need, we can move to creating the image encoder first. This is going to take care of the initial encoding of the images, mapping the original `28x28` matrix into `n` output features. As it will become clearer in the following, the dimension of the extracted features, `n`, has to be consistent with the quantum classification head. For instance, as in this example we are going to use a simple phase encoding to encode the features in rotation angles on the different qubits (one rotation for each qubit), we will have that `n` equals the number of qubits of the variational circuit.\n",
    "\n",
    "The image encoder is a purely classical layer, composed by a convolution, a pooling and a bunch of linear layers. Everything is implemented trhough the native `pytorch` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7143a5f-2066-4cbf-a7a7-375a77d457d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# we can define a classical image encoder\n",
    "# composed of a combination of convolutional, pooling and linear layers\n",
    "\n",
    "# number of extracted output features\n",
    "n_out_features = 2\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(3 * 13 * 13, 32)\n",
    "        self.fc3 = nn.Linear(32, n_out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = torch.flatten(x) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# we cast the layer to double precision, as this is the \n",
    "# default dtype used in qiboml, and more in general \n",
    "# for quantum simulation\n",
    "# this could could also be achieved by setting the torch \n",
    "# default dtype at the beginning with `torch.set_default_dtype(torch.float64)`\n",
    "img_encoder = ImageEncoder().double()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1f4c57-393e-48ea-8692-981a3e1eadff",
   "metadata": {},
   "source": [
    "## Define the Quantum model\n",
    "\n",
    "On top of the classical image encoder we are going to build a quantum classification layer. This can be done by using the `pytorch` interface provided by `qiboml`, namely the `qiboml.models.pytorch.QuantumModel`. In particular, to build a quantum model we need to specify three different components:\n",
    "\n",
    "- the **encoding**: how the classical data is encoded into the quantum circuit\n",
    "- the **circuit**: the actual variational circuit that we train\n",
    "- the **decoding**: how the quantum information is decoded to recover a classical output\n",
    "\n",
    "`qiboml` provides some predefined standard layers for those in the three modules `qiboml.models.encoding`, `qiboml.models.decoding` and `qiboml.models.ansatze`. For example, we could use:\n",
    "\n",
    "- **PhaseEncoding**: encodes the input features in `RY` rotations, one rotation on a different qubit for each feature\n",
    "- **Expectation**: decodes the quantum information by taking the expectation value of an observable on the final state after circuit execution\n",
    "\n",
    "and combine them with a custom variational circuit containing some parametrized rotations and entangling gates that we manually define.\n",
    "\n",
    "The `QuantumModel` can then be constructed by passing the three components `QuantumModel(encoding, circuit, decoding)`.\n",
    "\n",
    "Note that, as normally in `qibo`, you can set a global backend for the circuit execution. The backend choice mainly affects the decoding and autodifferentiation process, as, depending on the backend, different settings might be or not be available and differentiation is performed in a different way. For the time being, in this simple example, we are just going to use the `pytorch` backend that allows for native pytorch differentiation with the pytorch interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a8c49-a9f6-4c59-b802-a3b6387c6e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from qibo import set_backend, Circuit, gates\n",
    "from qibo.symbols import Z\n",
    "from qibo.hamiltonians import SymbolicHamiltonian\n",
    "\n",
    "from qiboml.interfaces.pytorch import QuantumModel\n",
    "from qiboml.models.encoding import PhaseEncoding\n",
    "from qiboml.models.decoding import Expectation\n",
    "\n",
    "# then we build the quantum model, that will\n",
    "# act as a classification layer on top of the\n",
    "# classical image encoder\n",
    "\n",
    "# let's set the qibo global backend with the one we like,\n",
    "# e.g. pytorch \n",
    "set_backend(backend=\"qiboml\", platform=\"pytorch\")\n",
    "\n",
    "# we prepare a quantum encoder to encode the \n",
    "# classical data in a quantum circuit\n",
    "# the number of qubits has to be equal to the number of\n",
    "# features extracted on the preceding layer, i.e. the\n",
    "# outputs of the image encoder\n",
    "nqubits = n_out_features\n",
    "encoding = PhaseEncoding(nqubits=nqubits)\n",
    "\n",
    "# we construct a trainable parametrized circuit\n",
    "# that is the core of our quantum model\n",
    "circuit = Circuit(nqubits)\n",
    "for _ in range(5):\n",
    "    for q in range(nqubits):\n",
    "        circuit.add(gates.RY(q, theta=np.random.randn() * np.pi))\n",
    "        circuit.add(gates.RZ(q, theta=np.random.randn() * np.pi))\n",
    "    circuit.add(gates.CNOT(0,1))\n",
    "print(\"Trainable Circuit:\")\n",
    "circuit.draw()\n",
    "\n",
    "# and finally we need a decoder to decode the quantum\n",
    "# information and extract the classical predictions,\n",
    "# for instance the expectation value calculation\n",
    "\n",
    "# for this we need to define the observable we wish to\n",
    "# measure\n",
    "observable = SymbolicHamiltonian(Z(0), nqubits=nqubits)\n",
    "# and then construct the expectation decoder\n",
    "decoding = Expectation(nqubits=nqubits, observable=observable)\n",
    "\n",
    "# we can then build the complete quantum model\n",
    "q_model = QuantumModel(\n",
    "    encoding,\n",
    "    circuit,\n",
    "    decoding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cbc6bc-1841-4a61-9ef1-85c8f367d481",
   "metadata": {},
   "source": [
    "## Set up a custom activation for the Quantum model\n",
    "\n",
    "The final ingredient that we miss is an activation for the quantum layer. This is not stricly mandatory, in the sense that the pipeline would still work regardless, but, depending on the situation, it is usually a good idea to control what kind of data we are encoding in our circuit. \n",
    "\n",
    "For example, in this case we are using a phase encoding, meaning that we encode the data directly in rotation angles on the Bloch sphere. However, the features extracted by the classical image encoder can in principle take values in $\\mathbb{R}^n$, while the rotations have a periodicity of $2\\pi$. Thus, taking for instance a rotation of $237.43$ is not very meaningful on its own, but more importantly, a small difference in such a large angle might still lead to a very different encoded quantum information, leading to a great asymmetry or incosistency between the classical and quantum layers that could make training the model very complicated. Note that the same is true in the opposite direction: a very large value but close to a multiple of $2\\pi$ (for instance $200 \\pi + \\epsilon$), would result in the exact same rotation as the small $\\epsilon$ signal.\n",
    "\n",
    "To put it into numbers, consider fo example the two features $237.43$ and $234.29$, while being very similar from the perspective of the image encoder, they account in practice for roughly a $\\pi$ rotation ($237.43 - 234.29 = 3.14$) for the quantum encoder. That's half of a full rotation on the Bloch sphere!\n",
    "\n",
    "Viceversa, two extremely different data for the classical image encoder, such as $3140$ ($\\sim 3000\\pi$) and $3.14$ ($\\sim \\pi$) would result in the exact same encoded quantum information.\n",
    "\n",
    "For this reason, in this case, we could define a custom activation that takes the outputs of the image encoder and maps them in the $(-\\pi, \\pi)$ interval through some nonlinear activation, e.g. the hyperbolic tangent. This again, is purely classical and thus we only rely on the pytorch API for the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c070bc-c58a-4703-97f3-699ee78fef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are using an angular encoding, it is better to\n",
    "# make sure that the output of the img_encoder are meaningful\n",
    "# angles. To do so we can define a custom torch activation\n",
    "# that rescales everything in the interval (-pi, pi)\n",
    "\n",
    "class PiTanh(nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        # we first rescale x to avoid the risk\n",
    "        # of saturating the tanh too often and, thus,\n",
    "        # producing always the same angle\n",
    "        x = x / x.max()\n",
    "        # then we just apply the tanh and rescale by pi\n",
    "        return np.pi * F.tanh(x)\n",
    "\n",
    "activation = PiTanh().double()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac807c5-e507-4f63-ae83-436cb3d96ae4",
   "metadata": {},
   "source": [
    "## Put everything together\n",
    "\n",
    "We can finally build the complete model by stacking the different layers on top of each other through the pytorch `Sequential` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac45461-78c3-48a0-95fc-9fafb5d14bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, the complete hybrid classical-quantum pipeline can\n",
    "# be built by stacking the different components in a \n",
    "# torch.nn.Sequential as usual\n",
    "model = nn.Sequential(\n",
    "    img_encoder,\n",
    "    activation,\n",
    "    q_model,\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f227264-d52c-46bf-a41b-ebdfd85cb9ed",
   "metadata": {},
   "source": [
    "## Baseline performance evaluation\n",
    "\n",
    "We are ready to run the model now!\n",
    "\n",
    "Let's start by checking how the model performs before any training happened, i.e. with just the random initialization.\n",
    "To do that we can use a standard classification metric, the [F1 score](https://en.wikipedia.org/wiki/F-score), that combines precision and recall in a single score.\n",
    "\n",
    "As the problem is a simple binary classification task, even random guessing would result in roughly 50% accuracy, thus nonzero performance shouldn't surprise you and may vary depending on the random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc298c92-88fd-44b4-91ef-efe6ebbae0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to evaluate the performance of the model we can use the\n",
    "# standard binary F1 score\n",
    "from torcheval.metrics.functional import binary_f1_score\n",
    "\n",
    "# let's check how the model does before training on the test set\n",
    "with torch.no_grad():\n",
    "    predictions = torch.as_tensor([F.sigmoid(model(x)) for x in test_data])\n",
    "    print(f\"Untrained F1 score: {binary_f1_score(predictions, test_targets)}\\n\")\n",
    "\n",
    "# we can also manually display and check some predictions and input images \n",
    "for prediction, image in zip(predictions[:5], test_images[:5]):\n",
    "    print(f\"Prediction: {prediction.round()}\\nImage:\")\n",
    "    display(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38cda1d-be5d-4b7a-a6fc-190760beaf85",
   "metadata": {},
   "source": [
    "## Train the model \n",
    "\n",
    "Now we can train the model on the train set as we would do normally for any other pytorch model. We use the `Adam` optimizer and train for 5 epochs, updating the parameters after each train sample. Thus we calculate $5 \\cdot 100$ losses in total and perform $500$ parameters update as well. \n",
    "\n",
    "To double check that the training is working we can plot the evolution of the loss over training. As each point of the plot consists the loss for a single sample of the train set, large variations are to be expected, but the overall trend should clearly be descending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b547bb43-bd97-40ee-b0f5-b765fffba02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "\n",
    "# now we can train the model as we would do\n",
    "# with any other pytorch model\n",
    "\n",
    "# let's use the torch.optim.Adam optimizer\n",
    "optimizer = Adam(model.parameters())\n",
    "# we are going to train for 5 epochs\n",
    "losses = []\n",
    "for _ in range(5):\n",
    "    # reshuffle the data before each epoch\n",
    "    permutation = torch.randperm(len(train_data))\n",
    "    for x, y in zip(train_data[permutation], train_targets[permutation]):\n",
    "        optimizer.zero_grad()\n",
    "        # get the predictions\n",
    "        out = model(x)\n",
    "        # calculate the loss, we can take the\n",
    "        # standard binary cross entropy\n",
    "        loss = F.binary_cross_entropy_with_logits(out.view(1,), y.view(1,))\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "# we can plot the training loss over the epochs\n",
    "plt.plot(range(len(losses)), losses)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "# for reference we calculate the average loss obtained for\n",
    "# the last 20 predictions after training\n",
    "print(f\"Final Loss: {sum(losses[-20:])/20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21034397-1744-41c2-a466-f55c70f64539",
   "metadata": {},
   "source": [
    "## Evaluate the trained model\n",
    "\n",
    "After training we can again evaluate the model on the test data through the F1 score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1c1d2a-0426-4e43-8ade-8f00f4d0b9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's double check that the F1 score has improved\n",
    "with torch.no_grad():\n",
    "    predictions = torch.as_tensor([F.sigmoid(model(x)) for x in test_data])\n",
    "    print(f\"Trained F1 score: {binary_f1_score(predictions, test_targets)}\")\n",
    "\n",
    "# once again, manual inspection of the predictions and the input images\n",
    "# should confirm the recorded score\n",
    "for prediction, image in zip(predictions[:5], test_images[:5]):\n",
    "    print(f\"Prediction: {prediction.round()}\\nImage:\")\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d40e59b-1f02-42b1-ac45-785f36134da7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
