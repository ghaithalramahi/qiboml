{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f929e9a9-d6c3-4857-b9b5-bc4f267955f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from qibo.symbols import Z\n",
    "from qibo import hamiltonians\n",
    "\n",
    "from qiboml.models.encoding_decoding import *\n",
    "from qiboml.models.ansatze import *\n",
    "\n",
    "def run_model(model, x):\n",
    "    for layer in model:\n",
    "        x = layer.forward(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09cccd24-dece-4279-bcd2-7006582458c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0. 0. 0. 1. 0. 0. 0.], shape=(8,), dtype=float64)\n",
      "tf.Tensor([0. 0. 1. 0. 0. 0. 0. 0.], shape=(8,), dtype=float64)\n",
      "tf.Tensor([0. 0. 0. 0. 1. 0. 0. 0.], shape=(8,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# binary data\n",
    "model = [BinaryEncodingLayer(nqubits=3), ReuploadingLayer(nqubits=3, qubits=(0,2)), QuantumDecodingLayer(nqubits=3, qubits=(1,0))]\n",
    "data = np.array([[0,0,0], [0,1,0], [1,1,0]])\n",
    "\n",
    "for x in data:\n",
    "    print(run_model(model, x).probabilities())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "005b4456-baab-486d-901a-0834ee042f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Qibo 0.2.6|INFO|2024-06-20 12:12:39]: Using qibojit (numba) backend on /CPU:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "tf.Tensor(1.0, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# real data\n",
    "nqubits = 4\n",
    "observable = hamiltonians.SymbolicHamiltonian(np.prod([Z(q) for q in range(nqubits)]))\n",
    "model = [PhaseEncodingLayer(nqubits, qubits=range(3)), ReuploadingLayer(nqubits), ExpectationLayer(nqubits, observable=observable)]\n",
    "data = np.pi * np.random.randn(3**2).reshape(3, 3)\n",
    "\n",
    "for x in data:\n",
    "    print(run_model(model, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb3f3599-b477-4725-8b27-16e5ba5c151f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Model: Sequential(\n",
      "  (0): Linear(in_features=128, out_features=5, bias=True)\n",
      "  (1): Sigmoid()\n",
      "  (2): BinaryEncodingLayer(nqubits=5, qubits=[0, 1, 2, 3, 4], circuit=<qibo.models.circuit.Circuit object at 0x35734a490>, initial_state=None, backend=tensorflow)\n",
      "  (3): ReuploadingLayer(nqubits=5, qubits=(0, 2, 4), circuit=<qibo.models.circuit.Circuit object at 0x354a67990>, initial_state=None, backend=tensorflow)\n",
      "  (4): QuantumDecodingLayer(nqubits=5, qubits=<range_iterator object at 0x357448d20>, circuit=<qibo.models.circuit.Circuit object at 0x35131c990>, initial_state=None, backend=tensorflow, nshots=1000)\n",
      ")\n",
      "> Parameters\n",
      "0.weight torch.Size([5, 128])\n",
      "0.bias torch.Size([5])\n",
      "3.ReuploadingLayer torch.Size([6, 1])\n",
      "> Outputs\n",
      "tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(32,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(32,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(32,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# pytorch interface\n",
    "import torch\n",
    "import qiboml.models.pytorch as pt\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(128,5),\n",
    "    torch.nn.Sigmoid(),\n",
    "    pt.BinaryEncodingLayer(5),\n",
    "    pt.ReuploadingLayer(nqubits=5, qubits=(0,2,4)),\n",
    "    pt.QuantumDecodingLayer(nqubits=5, qubits=reversed(range(5)))\n",
    ")\n",
    "\n",
    "print(f\"> Model: {model}\")\n",
    "print(\"> Parameters\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)\n",
    "\n",
    "data = torch.randn(3,128)\n",
    "print(\"> Outputs\")\n",
    "for x in data:\n",
    "    print(model(x).probabilities())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b43a1c4b-2186-4022-bf75-97d559d68d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Model: <Sequential name=sequential, built=False>\n",
      "> Outputs\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "Exception encountered when calling Model.call().\n\n\u001b[1mIterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by Model.call():\n  • args=('<KerasTensor shape=(1, 5), dtype=float32, sparse=False, name=keras_tensor_1>',)\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mprobabilities())\n",
      "File \u001b[0;32m~/python_envs/qibo/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/git/qiboml/src/qiboml/models/keras.py:77\u001b[0m, in \u001b[0;36mModel.call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 77\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/git/qiboml/src/qiboml/models/encoding_decoding.py:30\u001b[0m, in \u001b[0;36mBinaryEncodingLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m circuit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcircuit\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     29\u001b[0m ones \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mravel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 30\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mones\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcircuit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgates\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqubits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m circuit\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: Exception encountered when calling Model.call().\n\n\u001b[1mIterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by Model.call():\n  • args=('<KerasTensor shape=(1, 5), dtype=float32, sparse=False, name=keras_tensor_1>',)\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "# keras/tensorflow interface\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import qiboml.models.keras as ks\n",
    "import qiboml.models.encoding_decoding as ed\n",
    "import qiboml.models.ansatze as ans\n",
    "\n",
    "nqubits = 5\n",
    "observable = hamiltonians.SymbolicHamiltonian(np.prod([Z(q) for q in range(nqubits)]))\n",
    "\n",
    "q_model = ks.Model(\n",
    "    layers = [\n",
    "        ed.BinaryEncodingLayer(5),\n",
    "        ans.ReuploadingLayer(nqubits=5, qubits=(0,2,4)),\n",
    "        ed.ExpectationLayer(nqubits=5, qubits=reversed(range(5)), observable=observable)\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(5, activation=\"sigmoid\", name=\"dense\"),\n",
    "    q_model,\n",
    "])\n",
    "\n",
    "\n",
    "print(f\"> Model: {model}\")\n",
    "\n",
    "data = tf.random.uniform((3,128))\n",
    "print(\"> Outputs\")\n",
    "for x in data:\n",
    "    x = tf.expand_dims(x, axis=0)\n",
    "    print(model(x).probabilities())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc58a6-bddd-4397-b402-895574989ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
